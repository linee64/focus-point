import re
from collections import Counter
from backend.services.gemini_service import gemini_service

class SummarizationService:
    def __init__(self):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (Knowledge Extractor)...")
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏)
        self.stop_words = set([
            "–∏", "–≤", "–≤–æ", "–Ω–µ", "—á—Ç–æ", "–æ–Ω", "–Ω–∞", "—è", "—Å", "—Å–æ", "–∫–∞–∫", "–∞", "—Ç–æ", "–≤—Å–µ", "–æ–Ω–∞", 
            "—Ç–∞–∫", "–µ–≥–æ", "–Ω–æ", "–¥–∞", "—Ç—ã", "–∫", "—É", "–∂–µ", "–≤—ã", "–∑–∞", "–±—ã", "–ø–æ", "—Ç–æ–ª—å–∫–æ", "–µ–µ", 
            "–º–Ω–µ", "–±—ã–ª–æ", "–≤–æ—Ç", "–æ—Ç", "–º–µ–Ω—è", "–µ—â–µ", "–Ω–µ—Ç", "–æ", "–∏–∑", "–µ–º—É", "—Ç–µ–ø–µ—Ä—å", "–∫–æ–≥–¥–∞", 
            "–¥–∞–∂–µ", "–Ω—É", "–≤–¥—Ä—É–≥", "–ª–∏", "–µ—Å–ª–∏", "—É–∂–µ", "–∏–ª–∏", "–Ω–∏", "–±—ã—Ç—å", "–±—ã–ª", "–Ω–µ–≥–æ", "–¥–æ", 
            "–≤–∞—Å", "–Ω–∏–±—É–¥—å", "–æ–ø—è—Ç—å", "—É–∂", "–≤–∞–º", "–≤–µ–¥—å", "—Ç–∞–º", "–ø–æ—Ç–æ–º", "—Å–µ–±—è", "–Ω–∏—á–µ–≥–æ", "–µ–π", 
            "–º–æ–∂–µ—Ç", "–æ–Ω–∏", "—Ç—É—Ç", "–≥–¥–µ", "–µ—Å—Ç—å", "–Ω–∞–¥–æ", "–Ω–µ–π", "–¥–ª—è", "–º—ã", "—Ç–µ–±—è", "–∏—Ö", "—á–µ–º", 
            "–±—ã–ª–∞", "—Å–∞–º", "—á—Ç–æ–±", "–±–µ–∑", "–±—É–¥—Ç–æ", "—á–µ–≥–æ", "—Ä–∞–∑", "—Ç–æ–∂–µ", "—Å–µ–±–µ", "–ø–æ–¥", "–±—É–¥–µ—Ç", 
            "–∂", "—Ç–æ–≥–¥–∞", "–∫—Ç–æ", "—ç—Ç–æ—Ç", "—Ç–æ–≥–æ", "–ø–æ—Ç–æ–º—É", "—ç—Ç–æ–≥–æ", "–∫–∞–∫–æ–π", "—Å–æ–≤—Å–µ–º", "–Ω–∏–º", "–∑–¥–µ—Å—å", 
            "—ç—Ç–æ–º", "–æ–¥–∏–Ω", "–ø–æ—á—Ç–∏", "–º–æ–π", "—Ç–µ–º", "—á—Ç–æ–±—ã", "–Ω—É–∂–Ω–æ", "–æ—á–µ–Ω—å", "–ø—Ä–æ—Å—Ç–æ", "–∫–∞–∫ –±—ã", "—Ç–∏–ø–∞", "–≤–æ–æ–±—â–µ", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–∫–∞–∂–µ—Ç—Å—è", "—Å–≤–æ–µ–≥–æ", "—Ä–æ–¥–∞",
            "–ø–ª—é—Å", "–º–∏–Ω—É—Å", "—Ä–∞–≤–Ω–æ", "—Ä–∞–≤–Ω—è–µ—Ç—Å—è", "–∏–∫—Å", "–∏–≥—Ä–µ–∫", "–∑–µ–¥", "—á–∏—Å–ª–æ", "—Ü–∏—Ñ—Ä–∞", "–æ—Ç–≤–µ—Ç", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç", "–ø–æ–ª—É—á–∏–º", "–Ω–∞–π—Ç–∏", "—Ä–µ—à–∏—Ç—å"
        ])

    def _extract_keywords(self, text, top_n=5):
        # –û—á–∏—Å—Ç–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
        filtered_words = [w for w in words if w not in self.stop_words]
        counts = Counter(filtered_words)
        return [w for w, c in counts.most_common(top_n)]

    def _clean_text(self, text):
        # –£–¥–∞–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏ —Ç–∏–ø–∏—á–Ω—ã–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è –±–ª–æ–≥–µ—Ä–æ–≤
        intro_patterns = [
            r'–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ\s+—Ä–µ–±—è—Ç–∞', r'–≤—Å–µ–º\s+–ø—Ä–∏–≤–µ—Ç', r'—Å\s+–≤–∞–º–∏\s+—Ä–µ–∞–ª—å–Ω—ã–µ\s+–≤–µ–Ω–∞—Ç–æ—Ä',
            r'—Å–µ–≥–æ–¥–Ω—è\s+–Ω–∞\s+—É—Ä–æ–∫–µ', r'–º—ã\s+—Ä–∞–∑–±–µ—Ä–µ–º', r'–¥–æ–±—Ä–æ\s+–ø–æ–∂–∞–ª–æ–≤–∞—Ç—å',
            r'–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å\s+–Ω–∞\s+–∫–∞–Ω–∞–ª', r'—Å—Ç–∞–≤—å—Ç–µ\s+–ª–∞–π–∫–∏'
        ]
        
        cleaned = text
        for pattern in intro_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.I)
            
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é (–ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –∏ —Ç.–¥.)
        cleaned = re.sub(r'\s+([.,!?])', r'\1', cleaned)
        cleaned = re.sub(r'([.,!?])(?=[^\s])', r'\1 ', cleaned)
        
        return cleaned.strip()

    def _is_math_noise(self, text):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —á–∏—Å—Ç–æ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        math_patterns = [
            r'\d+\s*(?:–ø–ª—é—Å|–º–∏–Ω—É—Å|—É–º–Ω–æ–∂–∏—Ç—å|—Ä–∞–∑–¥–µ–ª–∏—Ç—å|—Ä–∞–≤–Ω–æ|—Ä–∞–≤–Ω—è–µ—Ç—Å—è|–ø–æ–ª—É—á–∏—Ç—Å—è|–≤\s+—Å—Ç–µ–ø–µ–Ω–∏|–≤\s+–∫–≤–∞–¥—Ä–∞—Ç–µ)',
            r'(?:–ø–ª—é—Å|–º–∏–Ω—É—Å|—É–º–Ω–æ–∂–∏—Ç—å|—Ä–∞–∑–¥–µ–ª–∏—Ç—å|—Ä–∞–≤–Ω–æ|—Ä–∞–≤–Ω—è–µ—Ç—Å—è|–ø–æ–ª—É—á–∏—Ç—Å—è)\s*\d+',
            r'\d+\s*[+\-*/=]\s*\d+',
            r'–∏–∫—Å\s+—Ä–∞–≤–Ω–æ', r'–∏–≥—Ä–µ–∫\s+—Ä–∞–≤–Ω–æ', r'–∑–µ–¥\s+—Ä–∞–≤–Ω–æ',
            r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ\s+—É\s+–Ω–∞—Å\s+–ø–æ–ª—É—á–∏—Ç—Å—è',
            r'—ç—Ç–æ\s+–±—É–¥–µ—Ç\s+–º–∏–Ω—É—Å\s+\d+',
            r'–≤–µ—Ä—à–∏–Ω–∞\s+–ø–∞—Ä–∞–±–æ–ª—ã\s+—É\s+–Ω–∞—Å\s+–±—É–¥–µ—Ç',
            r'\d+\s+—É–º–Ω–æ–∂–∏—Ç—å\s+–Ω–∞\s+\d+',
            r'–∫–≤–∞–¥—Ä–∞—Ç–µ\s+–º–∏–Ω—É—Å\s+\d+',
            r'\d+\s+–∏\s+–º–∏–Ω—É—Å\s+\d+', # –ü–∞—Ç—Ç–µ—Ä–Ω "3 –∏ –º–∏–Ω—É—Å 4"
            r'—Ç–æ—á–∫—É\s+–Ω–∞—à–ª–∏', r'–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã', r'–æ—Å—å\s+–∏–∫—Å', r'–æ—Å—å\s+–∏–≥—Ä–µ–∫'
        ]
        
        text_lower = text.lower()
        
        # –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –±—É–∫–≤–∞–º - —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ä–∞—Å—á–µ—Ç
        digits = len(re.findall(r'\d', text))
        letters = len(re.findall(r'[–∞-—è—ë]', text_lower))
        if digits > 0 and letters > 0 and (digits / letters) > 0.3:
            return True

        matches = 0
        for pattern in math_patterns:
            if re.search(pattern, text_lower):
                matches += 1
        
        return matches >= 1

    async def summarize_with_ai(self, text: str) -> dict:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Å–ø–µ–∫—Ç —Å –ø–æ–º–æ—â—å—é Gemini AI."""
        if not text or len(text.strip()) < 50:
            return {"title": "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", "summary": "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞."}

        prompt = f"""
–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –æ–±—É—á–µ–Ω–∏—é. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å—É–±—Ç–∏—Ç—Ä–æ–≤ –∏–∑ –≤–∏–¥–µ–æ).

**–í–ê–ñ–ù–û–ï –¢–†–ï–ë–û–í–ê–ù–ò–ï –ö –§–û–†–ú–ê–¢–£:**
–¢–≤–æ–π –æ—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω —Å–æ—Å—Ç–æ—è—Ç—å –∏–∑ –¥–≤—É—Ö —á–∞—Å—Ç–µ–π:
1. –ù–∞ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ –Ω–∞–ø–∏—à–∏ –¢–û–õ–¨–ö–û –∫—Ä–∞—Ç–∫–æ–µ –∏ –ø–æ–Ω—è—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã (–Ω–µ –±–æ–ª–µ–µ 10 —Å–ª–æ–≤).
2. –ù–∞—á–∏–Ω–∞—è —Å–æ –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏, –Ω–∞–ø–∏—à–∏ —Å–∞–º –ø–æ–¥—Ä–æ–±–Ω—ã–π –∫–æ–Ω—Å–ø–µ–∫—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.

**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Å–ø–µ–∫—Ç–∞:**
1. –û–ø—Ä–µ–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ –¥–∞–π –∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è.
2. –†–∞–∑–±–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏.
3. –ò—Å–ø–æ–ª—å–∑—É–π —Å–ø–∏—Å–∫–∏ –¥–ª—è –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤.
4. –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –∏–ª–∏ –≤—ã–≤–æ–¥—ã, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∏—Ö.
5. –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª, —É—Ä–∞–≤–Ω–µ–Ω–∏–π –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç LaTeX.
   - **–í–ê–ñ–ù–û**: –ö–∞–∂–¥–∞—è —Ñ–æ—Ä–º—É–ª–∞ –∏–ª–∏ —Å–∏–º–≤–æ–ª LaTeX –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ—Ç–¥–µ–ª–µ–Ω—ã –ø—Ä–æ–±–µ–ª–∞–º–∏ –æ—Ç –¥—Ä—É–≥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä: "–ø—É—Å—Ç—å ( $x$ ) –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç ( $\mathbb{{R}}$ )" –∏–ª–∏ "–≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ ( $-\infty; +\infty$ )".
   - –ò—Å–ø–æ–ª—å–∑—É–π –æ–¥–∏–Ω–æ—á–Ω—ã–µ –¥–æ–ª–ª–∞—Ä—ã ` $...$ ` –¥–ª—è —Ñ–æ—Ä–º—É–ª –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–æ–∫–∏.
   - –ò—Å–ø–æ–ª—å–∑—É–π –¥–≤–æ–π–Ω—ã–µ –¥–æ–ª–ª–∞—Ä—ã `$$...$$` –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ.
   - –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è: `$\infty$` –¥–ª—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–∏, `$\mathbb{{R}}$` –¥–ª—è –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∏—Å–µ–ª, `$\neq$` –¥–ª—è "–Ω–µ —Ä–∞–≤–Ω–æ", `$\pm$` –¥–ª—è "–ø–ª—é—Å-–º–∏–Ω—É—Å" –∏ —Ç.–¥.
   - –ü–∏—à–∏ —Ñ–æ—Ä–º—É–ª—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —á–∏—Å—Ç–æ, –∫–∞–∫ –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–µ.
6. –ü–∏—à–∏ –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞ —Ç–µ–∫—Å—Ç–∞ (–≤ –¥–∞–Ω–Ω–æ–º —Å–ª—É—á–∞–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º).

**–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:**
{text}

**–û—Ç–≤–µ—Ç (–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ 1-–π —Å—Ç—Ä–æ–∫–µ, –∑–∞—Ç–µ–º –∫–æ–Ω—Å–ø–µ–∫—Ç):**
"""
        try:
            response = await gemini_service.get_response(prompt)
            if "–û—à–∏–±–∫–∞:" in response or "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é" in response:
                print(f"Gemini –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä. –û—à–∏–±–∫–∞: {response}")
                return self.summarize(text)
            
            lines = response.strip().split('\n')
            title = lines[0].strip().strip('#').strip('*').strip()
            summary = '\n'.join(lines[1:]).strip()
            
            if not summary: # –ï—Å–ª–∏ –ò–ò –Ω–µ —Ä–∞–∑–¥–µ–ª–∏–ª –Ω–∞ —Å—Ç—Ä–æ–∫–∏
                summary = response
                title = self._extract_keywords(text)[0].capitalize() if self._extract_keywords(text) else "–ö–æ–Ω—Å–ø–µ–∫—Ç –≤–∏–¥–µ–æ"

            return {"title": title, "summary": summary}
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Gemini: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ç–æ—Ä.")
            return self.summarize(text)

    def summarize(self, text: str) -> dict:
        if not text or len(text.strip()) < 50:
            return {"title": "–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç", "summary": "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞."}

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = self._clean_text(text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        raw_sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = []
        for s in raw_sentences:
            s = s.strip()
            if len(s) > 20 and not self._is_math_noise(s):
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞ –∑–Ω–∞–∫–æ–≤ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                s = re.sub(r'[ ,;:-]+$', '', s)
                if not s.endswith(('.', '!', '?')):
                    s += '.'
                sentences.append(s)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ)
        keywords = self._extract_keywords(text)
        
        definitions = []
        key_aspects = []
        theory = []
        
        # –ú–∞—Ä–∫–µ—Ä—ã
        def_markers = [
            (r'\s+‚Äî\s+', ' ‚Äî '), 
            (r'\s+—ç—Ç–æ\s+', ' —ç—Ç–æ '), 
            (r'\s+—è–≤–ª—è–µ—Ç—Å—è\s+', ' —è–≤–ª—è–µ—Ç—Å—è '), 
            (r'\s+–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\s+', ' –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è ')
        ]
        aspect_markers = ['–≤–∞–∂–Ω–æ', '–æ—Å–Ω–æ–≤–Ω–æ–µ', '–≥–ª–∞–≤–Ω–æ–µ', '–ø—Ä–∏–Ω—Ü–∏–ø', '–ø—Ä–∞–≤–∏–ª–æ', '–∑–∞–ø–æ–º–Ω–∏—Ç–µ', '—Å—É—Ç—å']
        logic_markers = ['–ø–æ—Ç–æ–º—É —á—Ç–æ', '—Ç–∞–∫ –∫–∞–∫', '—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ', '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ']

        for sent in sentences:
            sent_lower = sent.lower()
            
            # 1. –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            is_def = False
            for pattern, display_marker in def_markers:
                if re.search(pattern, sent_lower):
                    parts = re.split(pattern, sent, maxsplit=1, flags=re.I)
                    if len(parts) > 1:
                        term = parts[0].strip()
                        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥ —Ç–µ—Ä–º–∏–Ω–æ–º –µ—Å—Ç—å "–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ", —É–±–∏—Ä–∞–µ–º —ç—Ç–æ
                        term = re.sub(r'^.*?(—á—Ç–æ|—á—Ç–æ–±—ã|–µ—Å–ª–∏|–∫–æ–≥–¥–∞)\s+', '', term, flags=re.I).capitalize()
                        desc = parts[1].strip().strip(' .')
                        
                        if 2 < len(term.split()) < 6 and len(desc.split()) > 2:
                            definitions.append(f"**{term}** ‚Äî {desc}.")
                            is_def = True
                            break
            if is_def: continue
                
            # 2. –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
            if any(marker in sent_lower for marker in aspect_markers):
                clean_a = re.sub(r'^(–∏—Ç–∞–∫|–≤ –æ–±—â–µ–º|–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ|–∫—Å—Ç–∞—Ç–∏|—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ),?\s*', '', sent, flags=re.I)
                key_aspects.append(clean_a.capitalize())
                continue
                
            # 3. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –∏ —Ç–µ–æ—Ä–∏—è
            if any(marker in sent_lower for marker in logic_markers) or any(kw in sent_lower for kw in keywords):
                if 30 < len(sent) < 250:
                    theory.append(sent.capitalize())

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ
        definitions = list(dict.fromkeys(definitions))[:15]
        key_aspects = list(dict.fromkeys(key_aspects))[:20]
        theory = list(dict.fromkeys(theory))[:20]

        # –°–±–æ—Ä–∫–∞ Markdown
        markdown = []
        title = keywords[0].capitalize() if keywords else "–ö–æ–Ω—Å–ø–µ–∫—Ç –≤–∏–¥–µ–æ"
        markdown.append(f"# üìö –ö–æ–Ω—Å–ø–µ–∫—Ç: {title}")

        if definitions:
            markdown.append("\n## üîç –ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è")
            markdown.extend([f"- {d}" for d in definitions])

        if key_aspects:
            markdown.append("\n## ‚ú® –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã")
            markdown.extend([f"- {a}" for a in key_aspects])

        if theory:
            markdown.append("\n## üìñ –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞")
            markdown.extend([f"- {t}" for t in theory])

        if not (definitions or key_aspects or theory):
            markdown.append("\n## üìã –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
            markdown.extend([f"- {s}" for s in sentences[:5]])

        return {"title": title, "summary": "\n".join(markdown)}
