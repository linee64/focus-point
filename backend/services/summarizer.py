import re
from collections import Counter

class SummarizationService:
    def __init__(self):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (Knowledge Extractor)...")
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏)
        self.stop_words = set([
            "–∏", "–≤", "–≤–æ", "–Ω–µ", "—á—Ç–æ", "–æ–Ω", "–Ω–∞", "—è", "—Å", "—Å–æ", "–∫–∞–∫", "–∞", "—Ç–æ", "–≤—Å–µ", "–æ–Ω–∞", 
            "—Ç–∞–∫", "–µ–≥–æ", "–Ω–æ", "–¥–∞", "—Ç—ã", "–∫", "—É", "–∂–µ", "–≤—ã", "–∑–∞", "–±—ã", "–ø–æ", "—Ç–æ–ª—å–∫–æ", "–µ–µ", 
            "–º–Ω–µ", "–±—ã–ª–æ", "–≤–æ—Ç", "–æ—Ç", "–º–µ–Ω—è", "–µ—â–µ", "–Ω–µ—Ç", "–æ", "–∏–∑", "–µ–º—É", "—Ç–µ–ø–µ—Ä—å", "–∫–æ–≥–¥–∞", 
            "–¥–∞–∂–µ", "–Ω—É", "–≤–¥—Ä—É–≥", "–ª–∏", "–µ—Å–ª–∏", "—É–∂–µ", "–∏–ª–∏", "–Ω–∏", "–±—ã—Ç—å", "–±—ã–ª", "–Ω–µ–≥–æ", "–¥–æ", 
            "–≤–∞—Å", "–Ω–∏–±—É–¥—å", "–æ–ø—è—Ç—å", "—É–∂", "–≤–∞–º", "–≤–µ–¥—å", "—Ç–∞–º", "–ø–æ—Ç–æ–º", "—Å–µ–±—è", "–Ω–∏—á–µ–≥–æ", "–µ–π", 
            "–º–æ–∂–µ—Ç", "–æ–Ω–∏", "—Ç—É—Ç", "–≥–¥–µ", "–µ—Å—Ç—å", "–Ω–∞–¥–æ", "–Ω–µ–π", "–¥–ª—è", "–º—ã", "—Ç–µ–±—è", "–∏—Ö", "—á–µ–º", 
            "–±—ã–ª–∞", "—Å–∞–º", "—á—Ç–æ–±", "–±–µ–∑", "–±—É–¥—Ç–æ", "—á–µ–≥–æ", "—Ä–∞–∑", "—Ç–æ–∂–µ", "—Å–µ–±–µ", "–ø–æ–¥", "–±—É–¥–µ—Ç", 
            "–∂", "—Ç–æ–≥–¥–∞", "–∫—Ç–æ", "—ç—Ç–æ—Ç", "—Ç–æ–≥–æ", "–ø–æ—Ç–æ–º—É", "—ç—Ç–æ–≥–æ", "–∫–∞–∫–æ–π", "—Å–æ–≤—Å–µ–º", "–Ω–∏–º", "–∑–¥–µ—Å—å", 
            "—ç—Ç–æ–º", "–æ–¥–∏–Ω", "–ø–æ—á—Ç–∏", "–º–æ–π", "—Ç–µ–º", "—á—Ç–æ–±—ã", "–Ω—É–∂–Ω–æ", "–æ—á–µ–Ω—å", "–ø—Ä–æ—Å—Ç–æ", "–∫–∞–∫ –±—ã", "—Ç–∏–ø–∞", "–≤–æ–æ–±—â–µ", "–Ω–∞–≤–µ—Ä–Ω–æ–µ", "–∫–∞–∂–µ—Ç—Å—è", "—Å–≤–æ–µ–≥–æ", "—Ä–æ–¥–∞",
            "–ø–ª—é—Å", "–º–∏–Ω—É—Å", "—Ä–∞–≤–Ω–æ", "—Ä–∞–≤–Ω—è–µ—Ç—Å—è", "–∏–∫—Å", "–∏–≥—Ä–µ–∫", "–∑–µ–¥", "—á–∏—Å–ª–æ", "—Ü–∏—Ñ—Ä–∞", "–æ—Ç–≤–µ—Ç", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç", "–ø–æ–ª—É—á–∏–º", "–Ω–∞–π—Ç–∏", "—Ä–µ—à–∏—Ç—å"
        ])

    def _extract_keywords(self, text, top_n=5):
        # –û—á–∏—Å—Ç–∫–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', text.lower())
        filtered_words = [w for w in words if w not in self.stop_words]
        counts = Counter(filtered_words)
        return [w for w, c in counts.most_common(top_n)]

    def _clean_text(self, text):
        # –£–¥–∞–ª—è–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏ —Ç–∏–ø–∏—á–Ω—ã–µ –≤—Å—Ç—É–ø–ª–µ–Ω–∏—è –±–ª–æ–≥–µ—Ä–æ–≤
        intro_patterns = [
            r'–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ\s+—Ä–µ–±—è—Ç–∞', r'–≤—Å–µ–º\s+–ø—Ä–∏–≤–µ—Ç', r'—Å\s+–≤–∞–º–∏\s+—Ä–µ–∞–ª—å–Ω—ã–µ\s+–≤–µ–Ω–∞—Ç–æ—Ä',
            r'—Å–µ–≥–æ–¥–Ω—è\s+–Ω–∞\s+—É—Ä–æ–∫–µ', r'–º—ã\s+—Ä–∞–∑–±–µ—Ä–µ–º', r'–¥–æ–±—Ä–æ\s+–ø–æ–∂–∞–ª–æ–≤–∞—Ç—å',
            r'–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å\s+–Ω–∞\s+–∫–∞–Ω–∞–ª', r'—Å—Ç–∞–≤—å—Ç–µ\s+–ª–∞–π–∫–∏'
        ]
        
        cleaned = text
        for pattern in intro_patterns:
            cleaned = re.sub(pattern, '', cleaned, flags=re.I)
            
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é (–ø—Ä–æ–±–µ–ª—ã –ø–µ—Ä–µ–¥ –∑–Ω–∞–∫–∞–º–∏ –∏ —Ç.–¥.)
        cleaned = re.sub(r'\s+([.,!?])', r'\1', cleaned)
        cleaned = re.sub(r'([.,!?])(?=[^\s])', r'\1 ', cleaned)
        
        return cleaned.strip()

    def _is_math_noise(self, text):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —á–∏—Å—Ç–æ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        math_patterns = [
            r'\d+\s*(?:–ø–ª—é—Å|–º–∏–Ω—É—Å|—É–º–Ω–æ–∂–∏—Ç—å|—Ä–∞–∑–¥–µ–ª–∏—Ç—å|—Ä–∞–≤–Ω–æ|—Ä–∞–≤–Ω—è–µ—Ç—Å—è|–ø–æ–ª—É—á–∏—Ç—Å—è|–≤\s+—Å—Ç–µ–ø–µ–Ω–∏|–≤\s+–∫–≤–∞–¥—Ä–∞—Ç–µ)',
            r'(?:–ø–ª—é—Å|–º–∏–Ω—É—Å|—É–º–Ω–æ–∂–∏—Ç—å|—Ä–∞–∑–¥–µ–ª–∏—Ç—å|—Ä–∞–≤–Ω–æ|—Ä–∞–≤–Ω—è–µ—Ç—Å—è|–ø–æ–ª—É—á–∏—Ç—Å—è)\s*\d+',
            r'\d+\s*[+\-*/=]\s*\d+',
            r'–∏–∫—Å\s+—Ä–∞–≤–Ω–æ', r'–∏–≥—Ä–µ–∫\s+—Ä–∞–≤–Ω–æ', r'–∑–µ–¥\s+—Ä–∞–≤–Ω–æ',
            r'—Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ\s+—É\s+–Ω–∞—Å\s+–ø–æ–ª—É—á–∏—Ç—Å—è',
            r'—ç—Ç–æ\s+–±—É–¥–µ—Ç\s+–º–∏–Ω—É—Å\s+\d+',
            r'–≤–µ—Ä—à–∏–Ω–∞\s+–ø–∞—Ä–∞–±–æ–ª—ã\s+—É\s+–Ω–∞—Å\s+–±—É–¥–µ—Ç',
            r'\d+\s+—É–º–Ω–æ–∂–∏—Ç—å\s+–Ω–∞\s+\d+',
            r'–∫–≤–∞–¥—Ä–∞—Ç–µ\s+–º–∏–Ω—É—Å\s+\d+',
            r'\d+\s+–∏\s+–º–∏–Ω—É—Å\s+\d+', # –ü–∞—Ç—Ç–µ—Ä–Ω "3 –∏ –º–∏–Ω—É—Å 4"
            r'—Ç–æ—á–∫—É\s+–Ω–∞—à–ª–∏', r'–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã', r'–æ—Å—å\s+–∏–∫—Å', r'–æ—Å—å\s+–∏–≥—Ä–µ–∫'
        ]
        
        text_lower = text.lower()
        
        # –ï—Å–ª–∏ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ü–∏—Ñ—Ä –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –±—É–∫–≤–∞–º - —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Ä–∞—Å—á–µ—Ç
        digits = len(re.findall(r'\d', text))
        letters = len(re.findall(r'[–∞-—è—ë]', text_lower))
        if digits > 0 and letters > 0 and (digits / letters) > 0.3:
            return True

        matches = 0
        for pattern in math_patterns:
            if re.search(pattern, text_lower):
                matches += 1
        
        return matches >= 1

    def summarize(self, text: str) -> str:
        if not text or len(text.strip()) < 50:
            return "–¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Å–ø–µ–∫—Ç–∞."

        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        text = self._clean_text(text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        raw_sentences = re.split(r'(?<=[.!?])\s+', text)
        sentences = []
        for s in raw_sentences:
            s = s.strip()
            if len(s) > 20 and not self._is_math_noise(s):
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞ –∑–Ω–∞–∫–æ–≤ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                s = re.sub(r'[ ,;:-]+$', '', s)
                if not s.endswith(('.', '!', '?')):
                    s += '.'
                sentences.append(s)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (—É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ)
        keywords = self._extract_keywords(text)
        
        definitions = []
        key_aspects = []
        theory = []
        
        # –ú–∞—Ä–∫–µ—Ä—ã
        def_markers = [
            (r'\s+‚Äî\s+', ' ‚Äî '), 
            (r'\s+—ç—Ç–æ\s+', ' —ç—Ç–æ '), 
            (r'\s+—è–≤–ª—è–µ—Ç—Å—è\s+', ' —è–≤–ª—è–µ—Ç—Å—è '), 
            (r'\s+–Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è\s+', ' –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è ')
        ]
        aspect_markers = ['–≤–∞–∂–Ω–æ', '–æ—Å–Ω–æ–≤–Ω–æ–µ', '–≥–ª–∞–≤–Ω–æ–µ', '–ø—Ä–∏–Ω—Ü–∏–ø', '–ø—Ä–∞–≤–∏–ª–æ', '–∑–∞–ø–æ–º–Ω–∏—Ç–µ', '—Å—É—Ç—å']
        logic_markers = ['–ø–æ—Ç–æ–º—É —á—Ç–æ', '—Ç–∞–∫ –∫–∞–∫', '—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ', '—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ']

        for sent in sentences:
            sent_lower = sent.lower()
            
            # 1. –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            is_def = False
            for pattern, display_marker in def_markers:
                if re.search(pattern, sent_lower):
                    parts = re.split(pattern, sent, maxsplit=1, flags=re.I)
                    if len(parts) > 1:
                        term = parts[0].strip()
                        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥ —Ç–µ—Ä–º–∏–Ω–æ–º –µ—Å—Ç—å "–¢–∞–∫–∂–µ —Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ", —É–±–∏—Ä–∞–µ–º —ç—Ç–æ
                        term = re.sub(r'^.*?(—á—Ç–æ|—á—Ç–æ–±—ã|–µ—Å–ª–∏|–∫–æ–≥–¥–∞)\s+', '', term, flags=re.I).capitalize()
                        desc = parts[1].strip().strip(' .')
                        
                        if 2 < len(term.split()) < 6 and len(desc.split()) > 2:
                            definitions.append(f"**{term}** ‚Äî {desc}.")
                            is_def = True
                            break
            if is_def: continue
                
            # 2. –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã
            if any(marker in sent_lower for marker in aspect_markers):
                clean_a = re.sub(r'^(–∏—Ç–∞–∫|–≤ –æ–±—â–µ–º|–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ|–∫—Å—Ç–∞—Ç–∏|—Å—Ç–æ–∏—Ç –æ—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ),?\s*', '', sent, flags=re.I)
                key_aspects.append(clean_a.capitalize())
                continue
                
            # 3. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –∏ —Ç–µ–æ—Ä–∏—è
            if any(marker in sent_lower for marker in logic_markers) or any(kw in sent_lower for kw in keywords):
                if 30 < len(sent) < 250:
                    theory.append(sent.capitalize())

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –≤–∏–¥–µ–æ
        definitions = list(dict.fromkeys(definitions))[:15]
        key_aspects = list(dict.fromkeys(key_aspects))[:20]
        theory = list(dict.fromkeys(theory))[:20]

        # –°–±–æ—Ä–∫–∞ Markdown
        markdown = []
        title = keywords[0].capitalize() if keywords else "–≤–∏–¥–µ–æ"
        markdown.append(f"# üìö –ö–æ–Ω—Å–ø–µ–∫—Ç: {title}")

        if definitions:
            markdown.append("\n## üîç –ö–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è")
            markdown.extend([f"- {d}" for d in definitions])

        if key_aspects:
            markdown.append("\n## ‚ú® –û—Å–Ω–æ–≤–Ω—ã–µ –∞—Å–ø–µ–∫—Ç—ã")
            markdown.extend([f"- {a}" for a in key_aspects])

        if theory:
            markdown.append("\n## üìñ –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞")
            markdown.extend([f"- {t}" for t in theory])

        if not (definitions or key_aspects or theory):
            markdown.append("\n## üìã –ö—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ")
            markdown.extend([f"- {s}" for s in sentences[:5]])

        return "\n".join(markdown)
